{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import Levenshtein as L\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('./dataset/train.npy', encoding='latin1')\n",
    "train_transcript_raw = np.load('./dataset/train_transcripts.npy')\n",
    "dev_data = np.load('./dataset/dev.npy', encoding='latin1')\n",
    "dev_transcript_raw = np.load('./dataset/dev_transcripts.npy')\n",
    "test_data = np.load('./dataset/test.npy', encoding='latin1')\n",
    "\n",
    "char_set = ['',\"'\",'+','-','.','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','_',' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_char(transcripts):\n",
    "    empty_idx, space_idx = 0, len(char_set)-1\n",
    "    char_representation = []\n",
    "    for line in transcripts:\n",
    "        char_repre = [empty_idx]\n",
    "        for w in line:\n",
    "            char_repre.extend([char_set.index(c) for c in list(w.decode('utf-8'))])\n",
    "            char_repre.append(space_idx)\n",
    "        if len(line) > 0:\n",
    "            char_repre.pop()\n",
    "        char_repre.append(empty_idx)\n",
    "        char_representation.append(np.array(char_repre))\n",
    "    return np.array(char_representation)\n",
    "\n",
    "train_transcript = preprocess_char(train_transcript_raw)\n",
    "dev_transcript = preprocess_char(dev_transcript_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LASDataLoader(DataLoader):\n",
    "    \n",
    "    def __init__(self, data, transcript, batch_size, shuffle=True):\n",
    "        for i in range(len(data)):\n",
    "            data[i] = torch.Tensor(data[i])\n",
    "            if transcript is None:\n",
    "                continue\n",
    "            transcript[i] = torch.LongTensor(transcript[i])\n",
    "        self.data = data\n",
    "        self.transcript = transcript\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            rand_idx = np.random.permutation(len(self.data))\n",
    "        else:\n",
    "            rand_idx = np.arange(len(self.data))\n",
    "        num_iter = len(self.data) // self.batch_size\n",
    "        if num_iter % self.batch_size != 0:\n",
    "            num_iter += 1\n",
    "        for i in range(num_iter):\n",
    "            batch_utter_list = self.data[rand_idx[i*self.batch_size:(i+1)*self.batch_size]]\n",
    "            utter_lens = [len(utter) for utter in batch_utter_list]\n",
    "            utter_order = sorted(range(len(utter_lens)), key=utter_lens.__getitem__, reverse=True)\n",
    "            batch_utter_list = [batch_utter_list[i] for i in utter_order]\n",
    "            \n",
    "            # for test data\n",
    "            if self.transcript is None:\n",
    "                yield batch_utter_list, utter_order\n",
    "            else:\n",
    "                batch_trans_list = self.transcript[rand_idx[i*self.batch_size:(i+1)*self.batch_size]]\n",
    "                trans_lens = [len(trans) for trans in batch_trans_list]\n",
    "                trans_order = sorted(range(len(trans_lens)), key=trans_lens.__getitem__, reverse=True)\n",
    "                batch_trans_data = [batch_trans_list[i][:-1] for i in trans_order]\n",
    "                batch_trans_label = [batch_trans_list[i][1:] for i in trans_order]\n",
    "                yield batch_utter_list, utter_order, batch_trans_data, batch_trans_label, trans_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "class WeightDrop(torch.nn.Module):\n",
    "    def __init__(self, module, weights, dropout=0):\n",
    "        super(WeightDrop, self).__init__()\n",
    "        self.module = module\n",
    "        self.weights = weights\n",
    "        self.dropout = dropout\n",
    "        self._setup()\n",
    "\n",
    "    def widget_demagnetizer_y2k_edition(*args, **kwargs):\n",
    "        # We need to replace flatten_parameters with a nothing function\n",
    "        # It must be a function rather than a lambda as otherwise pickling explodes\n",
    "        # We can't write boring code though, so ... WIDGET DEMAGNETIZER Y2K EDITION!\n",
    "        # (╯°□°）╯︵ ┻━┻\n",
    "        return\n",
    "\n",
    "    def _setup(self):\n",
    "        # Terrible temporary solution to an issue regarding compacting weights re: CUDNN RNN\n",
    "        if issubclass(type(self.module), torch.nn.RNNBase):\n",
    "            self.module.flatten_parameters = self.widget_demagnetizer_y2k_edition\n",
    "\n",
    "        for name_w in self.weights:\n",
    "            print('Applying weight drop of {} to {}'.format(self.dropout, name_w))\n",
    "            w = getattr(self.module, name_w)\n",
    "            del self.module._parameters[name_w]\n",
    "            self.module.register_parameter(name_w + '_raw', Parameter(w.data))\n",
    "\n",
    "    def _setweights(self):\n",
    "        for name_w in self.weights:\n",
    "            raw_w = getattr(self.module, name_w + '_raw')\n",
    "            w = torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training)\n",
    "            setattr(self.module, name_w, w)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Referenced from https://github.com/salesforce/awd-lstm-lm\n",
    "\n",
    "class LockedDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, dropout=0.5):\n",
    "        if not self.training or not dropout:\n",
    "            return x\n",
    "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
    "        mask = Variable(m, requires_grad=False) / (1 - dropout)\n",
    "        mask = mask.expand_as(x)\n",
    "        return mask * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channel, output_channel, kernel_size, stride, padding):\n",
    "        super(MaskConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channel, output_channel, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(output_channel)\n",
    "        self.tanh = nn.Hardtanh(0, 20, inplace=True)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.conv.weight)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "    \n",
    "    def forward(self, x , input_length):\n",
    "        # calculate the output length on seq dim\n",
    "        output_length = ((input_length + 2 * self.conv.padding[1] - self.conv.kernel_size[1]) / self.conv.stride[1] + 1)\n",
    "        \n",
    "        x = self.tanh(self.bn(self.conv(x)))\n",
    "        mask = torch.ByteTensor(x.size()).fill_(0)\n",
    "        if x.is_cuda:\n",
    "            mask = mask.to(DEVICE)\n",
    "        for i, length in enumerate(output_length):\n",
    "            length = length.item()\n",
    "            if (mask[i].size(2) - length) > 0:\n",
    "                mask[i].narrow(2, length, mask[i].size(2) - length).fill_(1)\n",
    "        x = x.masked_fill(mask, 0)\n",
    "        return x, output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sample\n",
    "class Beam:\n",
    "    def __init__(self, beam, log_prob, hidden_states, attention):\n",
    "        self.beam = beam\n",
    "        self.log_prob = log_prob\n",
    "        self.hidden_states = hidden_states\n",
    "        self.attention = attention\n",
    "\n",
    "class BeamSearch:\n",
    "    def __init__(self, beam_size):\n",
    "        self.beam_size = beam_size\n",
    "        self.top_beams = []\n",
    "        self.complete_beams = []\n",
    "    \n",
    "    def last_top_beams(self):\n",
    "        x = []\n",
    "        hidden = [None, None, None]\n",
    "        att = []\n",
    "        for beam in self.top_beams:\n",
    "            x.append(beam.beam[-1])\n",
    "            for layer in range(3):\n",
    "                if hidden[layer] is None:\n",
    "                    hidden[layer] = beam.hidden_states[layer]\n",
    "                else:\n",
    "                    hidden[layer] = (torch.cat((hidden[layer][0], beam.hidden_states[layer][0])), \n",
    "                                     torch.cat((hidden[layer][1], beam.hidden_states[layer][1])))\n",
    "            att.append(beam.attention)\n",
    "        return torch.LongTensor(x).to(DEVICE), hidden, torch.cat(att)\n",
    "                \n",
    "    def update_beams(self, probs, attention, hidden_states):\n",
    "        vocab_size = probs.size(1)\n",
    "\n",
    "        candidate_beams = []\n",
    "        for i, beam in enumerate(self.top_beams):\n",
    "            for ind in range(vocab_size):\n",
    "                new_beam = beam.beam.copy()\n",
    "                new_beam.append(ind)\n",
    "                log_prob = beam.log_prob + probs[i][ind]\n",
    "                hidden = [(hidden_state[0][i].view(1,-1), \n",
    "                           hidden_state[1][i].view(1,-1)) for hidden_state in hidden_states]\n",
    "                att = attention[i]\n",
    "                candidate_beams.append(Beam(new_beam, log_prob, hidden, att))\n",
    "        candidate_scores = [beam.log_prob for beam in candidate_beams]\n",
    "        top_k_prob, top_k_ind = torch.topk(torch.Tensor(candidate_scores), min(len(candidate_scores), 2*self.beam_size))\n",
    "        \n",
    "        top_k_beams = []\n",
    "        k = 0\n",
    "        for ind in top_k_ind:\n",
    "            if candidate_beams[ind].beam[-1] == 0:\n",
    "                self.complete_beams.append(candidate_beams[ind])\n",
    "            else:\n",
    "                top_k_beams.append(candidate_beams[ind])\n",
    "                k += 1\n",
    "            if k == self.beam_size:\n",
    "                break\n",
    "        \n",
    "        self.top_beams = top_k_beams\n",
    "#         print (len(self.top_beams), len(self.complete_beams))\n",
    "    \n",
    "    def best_beam(self):\n",
    "        if len(self.complete_beams) == 0:\n",
    "            self.complete_beams = self.top_beams\n",
    "        top_scores = [beam.log_prob / len(beam.beam) for beam in self.complete_beams]\n",
    "        top_k_prob, top_k_ind = torch.topk(torch.Tensor(top_scores), min(len(top_scores), self.beam_size))\n",
    "#         print (\"prob: \", top_k_prob[0])\n",
    "        return self.complete_beams[top_k_ind[0]].beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pBLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, weight_drop):\n",
    "        super(pBLSTM, self).__init__()\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2,1))\n",
    "        self.blstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, bidirectional=True, batch_first=True)\n",
    "        if weight_drop:\n",
    "            self.blstm = WeightDrop(self.blstm, ['weight_hh_l0'], dropout=0.3)\n",
    "    \n",
    "    def forward(self, x, input_length, hidden_states): #B * L * F\n",
    "        batch_size, seq_len, feat_len = x.shape\n",
    "        out = x[:,:seq_len//2*2,:]\n",
    "        out = out.view(batch_size, seq_len//2, 2, feat_len)\n",
    "        out = self.pool(out).squeeze()\n",
    "        input_length //= 2\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, input_length, batch_first=True)\n",
    "        out, hidden = self.blstm(out, hidden_states)\n",
    "        out, output_length = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        return out, hidden, output_length\n",
    "\n",
    "class Listener(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, nlayers, key_size, value_size, weight_drop=True):\n",
    "        super(Listener, self).__init__()\n",
    "        self.hidden_states = nn.ParameterList([nn.Parameter(torch.zeros(2, 1, hidden_size)) for i in range(nlayers)])\n",
    "        self.cell_states = nn.ParameterList([nn.Parameter(torch.zeros(2, 1, hidden_size)) for i in range(nlayers)])\n",
    "            \n",
    "        self.conv1 = MaskConv(1, 32, kernel_size=(3, 3), stride=(2,1), padding=(1,1))\n",
    "        self.conv2 = MaskConv(32, 32, kernel_size=(3, 3), stride=(1,1), padding=(1,1))\n",
    "        self.conv3 = MaskConv(32, 32, kernel_size=(3, 3), stride=(1,1), padding=(1,1))\n",
    "        rnn_input_size = 32 * 20\n",
    "        \n",
    "        self.plstms = []\n",
    "        plstm = pBLSTM(rnn_input_size, hidden_size, weight_drop)\n",
    "        self.plstms.append(plstm)\n",
    "        self.add_module('plstm_0', plstm)\n",
    "        for i in range(nlayers-1):\n",
    "            plstm = pBLSTM(2*hidden_size, hidden_size, weight_drop)\n",
    "            self.plstms.append(plstm)\n",
    "            self.add_module('plstm_'+str(i+1), plstm)\n",
    "    \n",
    "        self.lockdrop = LockedDropout()\n",
    "        self.fc_key_proj = nn.Linear(2*hidden_size, key_size)\n",
    "        self.fc_value_proj = nn.Linear(2*hidden_size, value_size)\n",
    "    \n",
    "        self.lockdrop = LockedDropout()\n",
    "        self.fc_key_proj = nn.Linear(2*hidden_size, key_size)\n",
    "        self.fc_value_proj = nn.Linear(2*hidden_size, value_size)\n",
    "        \n",
    "    def forward(self, x, input_length):\n",
    "        x = x.transpose(1,2) #n f s\n",
    "        x = x.unsqueeze(1)\n",
    "        x, output_length = self.conv1(x, input_length)\n",
    "        x, output_length = self.conv2(x, output_length)\n",
    "        x, output_length = self.conv3(x, output_length)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3]) #n f s\n",
    "        x = x.transpose(1,2) # n s f\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        hidden_states = [(hidden_state.expand(2, batch_size, -1).contiguous(), \n",
    "                          cell_state.expand(2, batch_size, -1).contiguous()) \n",
    "                         for hidden_state, cell_state in zip(self.hidden_states, self.cell_states)]\n",
    "        for i, plstm in enumerate(self.plstms):\n",
    "            x = self.lockdrop(x, 0.3)\n",
    "            x, hidden, output_length = plstm(x, output_length, hidden_states[i])\n",
    "        x = self.lockdrop(x, 0.3)\n",
    "        key = self.fc_key_proj(x)\n",
    "        value = self.fc_value_proj(x)\n",
    "        return key, value, output_length\n",
    "\n",
    "class Speller(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, key_size, value_size, nlayers, weight_drop=False):\n",
    "        super(Speller, self).__init__()\n",
    "        self.nlayers = nlayers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.projection = nn.Linear(hidden_size, key_size)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.lockdrop = LockedDropout()\n",
    "        \n",
    "        self.hidden_states = nn.ParameterList([nn.Parameter(torch.zeros(1, hidden_size)) for i in range(nlayers)])\n",
    "        self.cell_states = nn.ParameterList([nn.Parameter(torch.zeros(1, hidden_size)) for i in range(nlayers)])\n",
    "        \n",
    "        self.rnn_cell_0 = nn.LSTMCell(input_size=embed_size+value_size, hidden_size=hidden_size)\n",
    "        if weight_drop:\n",
    "            self.rnn_cell_0 = WeightDrop(self.rnn_cell_0, ['weight_hh'], dropout=0.3)\n",
    "            \n",
    "        self.rnn_cells = []\n",
    "        for i in range(nlayers-1):\n",
    "            rnn_cell = nn.LSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "            if weight_drop:\n",
    "                rnn_cell = WeightDrop(rnn_cell, ['weight_hh'], dropout=0.3)\n",
    "            self.rnn_cells.append(rnn_cell)\n",
    "            self.add_module('rnn_cell_'+str(i+1), rnn_cell)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size+value_size, vocab_size)\n",
    "        \n",
    "    def step(self, h, last_hidden_states, att_key, att_value, utter_mask):\n",
    "        h = self.lockdrop(h.unsqueeze(1), 0.2).squeeze(1)\n",
    "        h, c = self.rnn_cell_0(h, last_hidden_states[0])\n",
    "        \n",
    "        hidden_states = [(h,c)]\n",
    "        for i, rnn_cell in enumerate(self.rnn_cells):\n",
    "            h = self.lockdrop(h.unsqueeze(1), 0.2).squeeze(1)\n",
    "            h, c = rnn_cell(h, last_hidden_states[i+1])\n",
    "            hidden_states.append((h,c))\n",
    "        \n",
    "        h = self.lockdrop(h.unsqueeze(1), 0.2).squeeze(1)\n",
    "        query = self.projection(h).unsqueeze(1)\n",
    "        energy = torch.bmm(query, att_key)\n",
    "        energy = self.softmax(energy)\n",
    "\n",
    "        # zero out pad for softmax calculation\n",
    "        att_mask = utter_mask.unsqueeze(1)\n",
    "        energy = energy * att_mask\n",
    "        sum_ = energy.sum(dim=2, keepdim=True)\n",
    "        energy = energy.div(sum_)\n",
    "        \n",
    "        att = torch.bmm(energy, att_value).squeeze(1)\n",
    "        concat_h = torch.cat((att, h), dim=1)\n",
    "        logits = self.fc(concat_h)\n",
    "        return logits, energy, att, hidden_states\n",
    "        \n",
    "    def forward(self, x, att_key, att_value, utter_input_length, teacher_force=1.0, plot=False):\n",
    "        batch_size, max_seq_len = att_key.size(0), att_key.size(1)\n",
    "        utter_mask = torch.FloatTensor(batch_size, max_seq_len).fill_(1).to(DEVICE)\n",
    "        for i, length in enumerate(utter_input_length):\n",
    "            length = length.item()\n",
    "            if length < max_seq_len:\n",
    "                utter_mask[i].narrow(0,length,max_seq_len-length).fill_(0)\n",
    "        utter_mask = utter_mask.unsqueeze(2)\n",
    "        att_key = att_key * utter_mask\n",
    "        att_value = att_value * utter_mask\n",
    "        \n",
    "        att_key = att_key.transpose(1,2)\n",
    "        output = []\n",
    "        max_seq_len = x.shape[1]\n",
    "        \n",
    "        x = self.embed(x)\n",
    "        assert(x.size(0) == att_value.size(0))\n",
    "        att = torch.zeros(att_value.size(0), att_value.size(2)).to(DEVICE)\n",
    "        hidden_states = [(hidden_state.expand(batch_size, -1), cell_state.expand(batch_size, -1)) \n",
    "                         for hidden_state, cell_state in zip(self.hidden_states, self.cell_states)]\n",
    "        \n",
    "        utter_mask = utter_mask.squeeze(2)\n",
    "        att_list = []\n",
    "        for i in range(max_seq_len):\n",
    "            indicator = np.random.random()\n",
    "            if indicator > teacher_force:\n",
    "                # use predicted chars\n",
    "                if i == 0:\n",
    "                    pred = torch.zeros(batch_size).long().to(DEVICE)\n",
    "                else:\n",
    "                    scores = F.gumbel_softmax(logits)\n",
    "                    _, pred = torch.max(scores, dim=1)\n",
    "                    pred = pred.long()\n",
    "                pred = self.embed(pred)\n",
    "                inp = torch.cat((pred, att), dim=-1)\n",
    "            else:\n",
    "                inp = torch.cat((x[:,i,:], att), dim=-1)\n",
    "            logits, energy, att, hidden_states = self.step(inp, hidden_states, att_key, att_value, utter_mask)\n",
    "            output.append(logits.unsqueeze(1))\n",
    "            if plot:\n",
    "                att_list.append(energy.squeeze(1).cpu().detach().numpy())\n",
    "        if plot:\n",
    "            self.plot_attention(att_list, utter_input_length)\n",
    "        return torch.cat(output, dim=1)\n",
    "    \n",
    "    def generate(self, att_key, att_value, utter_input_length, beam_search=True):\n",
    "        batch_size, max_seq_len = att_key.size(0), att_key.size(1)\n",
    "        x = torch.zeros(batch_size).long().to(DEVICE)\n",
    "        \n",
    "        utter_mask = torch.FloatTensor(batch_size, max_seq_len).fill_(1).to(DEVICE)\n",
    "        for i, length in enumerate(utter_input_length):\n",
    "            length = length.item()\n",
    "            if length < max_seq_len:\n",
    "                utter_mask[i].narrow(0,length,max_seq_len-length).fill_(0)\n",
    "        utter_mask = utter_mask.unsqueeze(2)\n",
    "        att_key = att_key * utter_mask\n",
    "        att_value = att_value * utter_mask\n",
    "        att_key = att_key.transpose(1,2)\n",
    "        \n",
    "        att = torch.zeros(att_value.size(0), att_value.size(2)).to(DEVICE)\n",
    "        hidden_states = [(hidden_state.expand(batch_size, -1), cell_state.expand(batch_size, -1)) \n",
    "                          for hidden_state, cell_state in zip(self.hidden_states, self.cell_states)]\n",
    "        \n",
    "        utter_mask = utter_mask.squeeze(2)\n",
    "\n",
    "        # for each sample\n",
    "#         print (att.shape, att_key.shape, att_value.shape)\n",
    "        output = []\n",
    "        beam_size = 32\n",
    "        for batch in range(batch_size):\n",
    "            beam_searcher = BeamSearch(beam_size)\n",
    "            init_input = [0]\n",
    "            init_hidden_states = [(hidden_state, cell_state) for hidden_state, cell_state\n",
    "                                  in zip(self.hidden_states, self.cell_states)]\n",
    "            init_att = att[batch]\n",
    "            init_beam = Beam(init_input, 0., init_hidden_states, init_att)\n",
    "            beam_searcher.top_beams.append(init_beam)\n",
    "            \n",
    "            step = 0\n",
    "            while (len(beam_searcher.complete_beams) < beam_size) and step < 300:\n",
    "                x, hidden_states, sample_att = beam_searcher.last_top_beams()\n",
    "                x = self.embed(x)\n",
    "                sample_att = sample_att.view(x.size(0),-1)\n",
    "                inp = torch.cat((x, sample_att), dim=-1)\n",
    "                \n",
    "                sample_att_key = att_key[batch].expand(len(x),-1,-1)\n",
    "                sample_att_value = att_value[batch].expand(len(x), -1,-1)\n",
    "                sample_utter_mask = utter_mask[batch].expand(len(x), -1)\n",
    "                \n",
    "                logits, energy, sample_att, hidden_states = self.step(inp, hidden_states, sample_att_key,\n",
    "                                                                      sample_att_value, sample_utter_mask)\n",
    "                scores = F.log_softmax(logits, dim=1)\n",
    "                beam_searcher.update_beams(scores, sample_att, hidden_states)\n",
    "                step += 1\n",
    "            output.append(beam_searcher.best_beam())\n",
    "            print ('finished with sample %d' % (batch+1))\n",
    "        return output\n",
    "    \n",
    "    def plot_attention(self, attention_list, utter_length):\n",
    "        print (attention_list[0].shape)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            attention = np.array([att[i] for att in attention_list])\n",
    "            attention = attention[:utter_length[i]]\n",
    "            attention /= np.linalg.norm(attention, axis=1, keepdims=True)\n",
    "            plt.imshow(attention, cmap='hot')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, listener, speller, train_loader, dev_loader, test_loader, max_epochs=1, run_id='exp'):\n",
    "        self.train_loader = train_loader\n",
    "        self.dev_loader = dev_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        self.listener = listener.to(DEVICE)\n",
    "        self.speller = speller.to(DEVICE)\n",
    "        \n",
    "        params = list(listener.parameters()) + list(speller.parameters())\n",
    "        self.optimizer = torch.optim.Adam(params, lr=3e-4, weight_decay=1e-6)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def update_lr(self, lr):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    def train_batch(self, batch_utter, utter_order, batch_trans_data, batch_trans_label, trans_order, \n",
    "                    pre_train, teacher_force=1.0, plot=False, generation=False):\n",
    "        if not pre_train:\n",
    "            utter_input_length = torch.IntTensor([len(utter) for utter in batch_utter])\n",
    "            batch_utter = nn.utils.rnn.pad_sequence(batch_utter, batch_first=True)\n",
    "            batch_utter = batch_utter.to(DEVICE)\n",
    "            att_key, att_value, utter_output_length = self.listener(batch_utter, utter_input_length)\n",
    "        \n",
    "            reorder = [utter_order.index(i) for i in trans_order]\n",
    "            att_key, att_value = att_key[reorder], att_value[reorder] \n",
    "            utter_output_length = utter_output_length[reorder]\n",
    "        else:\n",
    "            random_seq_len = 100\n",
    "            att_key = torch.zeros([len(batch_trans_data), random_seq_len, 128]).to(DEVICE)\n",
    "            att_value = torch.zeros([len(batch_trans_data), random_seq_len, 128]).to(DEVICE)\n",
    "            utter_output_length = torch.IntTensor([random_seq_len]*BATCH_SIZE)\n",
    "\n",
    "        trans_input_length = [len(trans) for trans in batch_trans_data]\n",
    "        batch_trans_data = nn.utils.rnn.pad_sequence(batch_trans_data, batch_first=True)\n",
    "        batch_trans_label = nn.utils.rnn.pack_sequence(batch_trans_label)\n",
    "        batch_trans_data = batch_trans_data.to(DEVICE)\n",
    "        batch_trans_label = batch_trans_label.to(DEVICE)\n",
    "        logits = self.speller(batch_trans_data, att_key, att_value, utter_output_length, \n",
    "                              teacher_force=teacher_force, plot=plot)\n",
    "    \n",
    "        logits = nn.utils.rnn.pack_padded_sequence(logits, trans_input_length, batch_first=True)\n",
    "        loss = self.criterion(logits.data, batch_trans_label.data)\n",
    "        return loss\n",
    "    \n",
    "    def train(self, pre_train=False, teacher_force=1.0):\n",
    "        self.listener.train()\n",
    "        self.speller.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch_utter, utter_order, batch_trans_data, batch_trans_label, trans_order in self.train_loader:\n",
    "            num_batches += 1\n",
    "            batch_loss = self.train_batch(batch_utter, utter_order, \n",
    "                                          batch_trans_data, batch_trans_label, trans_order,\n",
    "                                          pre_train, teacher_force=teacher_force)\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            batch_loss = batch_loss.item()\n",
    "            epoch_loss += batch_loss\n",
    "            if num_batches % 50 == 0:\n",
    "                print ('[TRAIN] Iter [%d/%d]    Perplexity: %.4f'\n",
    "                      % (num_batches, len(train_data) // BATCH_SIZE, np.exp(batch_loss)))\n",
    "            torch.cuda.empty_cache()\n",
    "        epoch_loss = epoch_loss / num_batches\n",
    "        self.epochs += 1\n",
    "        print ('[TRAIN] Epoch [%d/%d]    Perplexity: %.4f'\n",
    "               % (self.epochs, self.max_epochs, np.exp(epoch_loss)))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "    \n",
    "    def val(self, pre_trained=False, plot=False):\n",
    "        self.listener.eval()\n",
    "        self.speller.eval()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_utter, utter_order, batch_trans_data, batch_trans_label, trans_order in self.dev_loader:\n",
    "                num_batches += 1\n",
    "                batch_loss = self.train_batch(batch_utter, utter_order, \n",
    "                                              batch_trans_data, batch_trans_label, trans_order,\n",
    "                                              pre_trained, plot=plot)\n",
    "                epoch_loss += batch_loss.item()\n",
    "                if plot:\n",
    "                    return None\n",
    "                #torch.cuda.empty_cache()\n",
    "            epoch_loss = epoch_loss / num_batches\n",
    "            print ('[VAL] Epoch [%d/%d]    Perplexity: %.4f'\n",
    "                   % (self.epochs, self.max_epochs, np.exp(epoch_loss)))\n",
    "            self.val_losses.append(epoch_loss)\n",
    "        return epoch_loss\n",
    "    \n",
    "    def test(self, val=False, random_search=False, beam_search=True):\n",
    "        self.listener.eval()\n",
    "        self.speller.eval()\n",
    "        with torch.no_grad():\n",
    "            if val:\n",
    "                L_distance = 0\n",
    "                num_sentences = 0\n",
    "                for batch_utter, utter_order, batch_trans_data, _, trans_order in self.dev_loader:\n",
    "                    utter_input_length = torch.IntTensor([len(utter) for utter in batch_utter])\n",
    "                    batch_utter = nn.utils.rnn.pad_sequence(batch_utter, batch_first=True)\n",
    "                    batch_utter = batch_utter.to(DEVICE)\n",
    "                    att_key, att_value, utter_output_length = self.listener(batch_utter, utter_input_length)\n",
    "\n",
    "                    reorder = [utter_order.index(i) for i in trans_order]\n",
    "                    att_key, att_value = att_key[reorder], att_value[reorder] \n",
    "                    utter_output_length = utter_output_length[reorder]\n",
    "\n",
    "                    output = self.speller.generate(att_key, att_value, utter_output_length, beam_search=beam_search)\n",
    "\n",
    "                    for i in range(len(batch_trans_data)):\n",
    "                        pred = ''.join([char_set[c] for c in output[i]])\n",
    "                        true = ''.join([char_set[c] for c in batch_trans_data[i]])\n",
    "                        L_distance += L.distance(pred, true)\n",
    "                        if i == 5:\n",
    "                            print (\"PRED: \", pred)\n",
    "                            print (\"TRUE: \", true)\n",
    "                    num_sentences += len(batch_trans_data)\n",
    "                    print (L_distance / num_sentences)\n",
    "\n",
    "                print ('[VAL] Epoch [%d/%d]    L distance: %.4f'\n",
    "                      % (self.epochs, self.max_epochs, L_distance / num_sentences))\n",
    "            else:\n",
    "                index = 0\n",
    "                f = open('./try_conv_1.12.csv', 'w')\n",
    "                wr = csv.writer(f, dialect='excel')\n",
    "                wr.writerow(['Id', 'Predicted'])\n",
    "                for batch_utter, utter_order in self.test_loader:\n",
    "                    utter_input_length = torch.IntTensor([len(utter) for utter in batch_utter])\n",
    "                    batch_utter = nn.utils.rnn.pad_sequence(batch_utter, batch_first=True)\n",
    "                    batch_utter = batch_utter.to(DEVICE)\n",
    "                    att_key, att_value, utter_output_length = self.listener(batch_utter, utter_input_length)\n",
    "                \n",
    "                    reorder = [utter_order.index(i) for i in range(len(batch_utter))]\n",
    "                    att_key, att_value = att_key[reorder], att_value[reorder] \n",
    "                    utter_output_length = utter_output_length[reorder]\n",
    "                    \n",
    "                    output = self.speller.generate(att_key, att_value, utter_output_length, beam_search=beam_search)\n",
    "                    \n",
    "                    for i in range(len(batch_utter)):\n",
    "                        pred = ''.join([char_set[c] for c in output[i]])\n",
    "                        wr.writerow([index, pred])\n",
    "                        index += 1\n",
    "                f.close()\n",
    "                print ('Finished predictions')\n",
    "    \n",
    "    def save(self, pre_trained=False):\n",
    "        listener_path = os.path.join('./conv_experiments', self.run_id, 'listener-{}.pt'.format(self.epochs))\n",
    "        speller_path = os.path.join('./conv_experiments', self.run_id, 'speller-{}.pt'.format(self.epochs))\n",
    "        if not pre_trained:\n",
    "            torch.save(self.listener.state_dict(), listener_path)\n",
    "        torch.save(self.speller.state_dict(), speller_path)\n",
    "\n",
    "    def load(self, listener_path, speller_path):\n",
    "        if listener_path:\n",
    "            self.listener.load_state_dict(torch.load(listener_path))\n",
    "            print (\"Loaded trained listener\")\n",
    "        self.speller.load_state_dict(torch.load(speller_path))\n",
    "        print (\"Loaded trained speller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic = 'train_3by3_no_norm_on_spell_with_pretrain_0.7tf'\n",
    "# run_id = topic + str(int(time.time()))\n",
    "# if not os.path.exists('./conv_experiments'):\n",
    "#     os.mkdir('./conv_experiments')\n",
    "# os.mkdir('./conv_experiments/%s' % run_id)\n",
    "# print(\"Saving models, predictions, and generated words to ./conv_experiments/%s\" % run_id)\n",
    "run_id = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "PRE_TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying weight drop of 0.3 to weight_hh_l0\n",
      "Applying weight drop of 0.3 to weight_hh_l0\n",
      "Applying weight drop of 0.3 to weight_hh_l0\n"
     ]
    }
   ],
   "source": [
    "listener = Listener(input_size=40, hidden_size=256, nlayers=3, key_size=128, value_size=128)\n",
    "speller = Speller(vocab_size=len(char_set), embed_size=128, hidden_size=512, key_size=128, value_size=128, nlayers=3)\n",
    "\n",
    "train_loader = LASDataLoader(train_data, train_transcript, BATCH_SIZE)\n",
    "dev_loader = LASDataLoader(dev_data, dev_transcript, BATCH_SIZE, shuffle=True)\n",
    "test_loader = LASDataLoader(test_data, None, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "trainer = Trainer(listener, speller, train_loader, dev_loader, test_loader, max_epochs=NUM_EPOCHS, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained listener\n",
      "Loaded trained speller\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vision/miniconda3/envs/dbh/lib/python3.6/site-packages/torch/nn/modules/module.py:477: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with sample 1\n",
      "finished with sample 2\n",
      "finished with sample 3\n",
      "finished with sample 4\n",
      "finished with sample 5\n",
      "finished with sample 6\n",
      "finished with sample 7\n",
      "finished with sample 8\n",
      "finished with sample 9\n",
      "finished with sample 10\n",
      "finished with sample 11\n",
      "finished with sample 12\n",
      "finished with sample 13\n",
      "finished with sample 14\n",
      "finished with sample 15\n",
      "finished with sample 16\n",
      "finished with sample 17\n",
      "finished with sample 18\n",
      "finished with sample 19\n",
      "finished with sample 20\n",
      "finished with sample 21\n",
      "finished with sample 22\n",
      "finished with sample 23\n",
      "finished with sample 24\n",
      "finished with sample 25\n",
      "finished with sample 26\n",
      "finished with sample 27\n",
      "finished with sample 28\n",
      "finished with sample 29\n",
      "finished with sample 30\n",
      "finished with sample 31\n",
      "finished with sample 32\n",
      "finished with sample 33\n",
      "finished with sample 34\n",
      "finished with sample 35\n",
      "finished with sample 36\n",
      "finished with sample 37\n",
      "finished with sample 38\n",
      "finished with sample 39\n",
      "finished with sample 40\n",
      "finished with sample 41\n",
      "finished with sample 42\n",
      "finished with sample 43\n",
      "finished with sample 44\n",
      "finished with sample 45\n",
      "finished with sample 46\n",
      "finished with sample 47\n",
      "finished with sample 48\n",
      "finished with sample 49\n",
      "finished with sample 50\n",
      "finished with sample 51\n",
      "finished with sample 52\n",
      "finished with sample 53\n",
      "finished with sample 54\n",
      "finished with sample 55\n",
      "finished with sample 56\n",
      "finished with sample 57\n",
      "finished with sample 58\n",
      "finished with sample 59\n",
      "finished with sample 60\n",
      "finished with sample 61\n",
      "finished with sample 62\n",
      "finished with sample 63\n",
      "finished with sample 64\n",
      "finished with sample 1\n",
      "finished with sample 2\n",
      "finished with sample 3\n",
      "finished with sample 4\n",
      "finished with sample 5\n",
      "finished with sample 6\n",
      "finished with sample 7\n",
      "finished with sample 8\n",
      "finished with sample 9\n",
      "finished with sample 10\n",
      "finished with sample 11\n",
      "finished with sample 12\n",
      "finished with sample 13\n",
      "finished with sample 14\n",
      "finished with sample 15\n",
      "finished with sample 16\n",
      "finished with sample 17\n",
      "finished with sample 18\n",
      "finished with sample 19\n",
      "finished with sample 20\n",
      "finished with sample 21\n",
      "finished with sample 22\n",
      "finished with sample 23\n",
      "finished with sample 24\n",
      "finished with sample 25\n",
      "finished with sample 26\n",
      "finished with sample 27\n",
      "finished with sample 28\n",
      "finished with sample 29\n",
      "finished with sample 30\n",
      "finished with sample 31\n",
      "finished with sample 32\n",
      "finished with sample 33\n",
      "finished with sample 34\n",
      "finished with sample 35\n",
      "finished with sample 36\n",
      "finished with sample 37\n",
      "finished with sample 38\n",
      "finished with sample 39\n",
      "finished with sample 40\n",
      "finished with sample 41\n",
      "finished with sample 42\n",
      "finished with sample 43\n",
      "finished with sample 44\n",
      "finished with sample 45\n",
      "finished with sample 46\n",
      "finished with sample 47\n",
      "finished with sample 48\n",
      "finished with sample 49\n",
      "finished with sample 50\n",
      "finished with sample 51\n",
      "finished with sample 52\n",
      "finished with sample 53\n",
      "finished with sample 54\n",
      "finished with sample 55\n",
      "finished with sample 56\n",
      "finished with sample 57\n",
      "finished with sample 58\n",
      "finished with sample 59\n",
      "finished with sample 60\n",
      "finished with sample 61\n",
      "finished with sample 62\n",
      "finished with sample 63\n",
      "finished with sample 64\n",
      "finished with sample 1\n",
      "finished with sample 2\n",
      "finished with sample 3\n",
      "finished with sample 4\n",
      "finished with sample 5\n",
      "finished with sample 6\n",
      "finished with sample 7\n",
      "finished with sample 8\n",
      "finished with sample 9\n",
      "finished with sample 10\n",
      "finished with sample 11\n",
      "finished with sample 12\n",
      "finished with sample 13\n",
      "finished with sample 14\n",
      "finished with sample 15\n",
      "finished with sample 16\n",
      "finished with sample 17\n",
      "finished with sample 18\n",
      "finished with sample 19\n",
      "finished with sample 20\n",
      "finished with sample 21\n",
      "finished with sample 22\n",
      "finished with sample 23\n",
      "finished with sample 24\n",
      "finished with sample 25\n",
      "finished with sample 26\n",
      "finished with sample 27\n",
      "finished with sample 28\n",
      "finished with sample 29\n",
      "finished with sample 30\n",
      "finished with sample 31\n",
      "finished with sample 32\n",
      "finished with sample 33\n",
      "finished with sample 34\n",
      "finished with sample 35\n",
      "finished with sample 36\n",
      "finished with sample 37\n",
      "finished with sample 38\n",
      "finished with sample 39\n",
      "finished with sample 40\n",
      "finished with sample 41\n",
      "finished with sample 42\n",
      "finished with sample 43\n",
      "finished with sample 44\n",
      "finished with sample 45\n",
      "finished with sample 46\n",
      "finished with sample 47\n",
      "finished with sample 48\n",
      "finished with sample 49\n",
      "finished with sample 50\n",
      "finished with sample 51\n",
      "finished with sample 52\n",
      "finished with sample 53\n",
      "finished with sample 54\n",
      "finished with sample 55\n",
      "finished with sample 56\n",
      "finished with sample 57\n",
      "finished with sample 58\n",
      "finished with sample 59\n",
      "finished with sample 60\n",
      "finished with sample 61\n",
      "finished with sample 62\n",
      "finished with sample 63\n",
      "finished with sample 64\n",
      "finished with sample 1\n",
      "finished with sample 2\n",
      "finished with sample 3\n",
      "finished with sample 4\n",
      "finished with sample 5\n",
      "finished with sample 6\n",
      "finished with sample 7\n",
      "finished with sample 8\n",
      "finished with sample 9\n",
      "finished with sample 10\n",
      "finished with sample 11\n",
      "finished with sample 12\n",
      "finished with sample 13\n",
      "finished with sample 14\n",
      "finished with sample 15\n",
      "finished with sample 16\n",
      "finished with sample 17\n",
      "finished with sample 18\n",
      "finished with sample 19\n",
      "finished with sample 20\n",
      "finished with sample 21\n",
      "finished with sample 22\n",
      "finished with sample 23\n",
      "finished with sample 24\n",
      "finished with sample 25\n",
      "finished with sample 26\n",
      "finished with sample 27\n",
      "finished with sample 28\n",
      "finished with sample 29\n",
      "finished with sample 30\n",
      "finished with sample 31\n",
      "finished with sample 32\n",
      "finished with sample 33\n",
      "finished with sample 34\n",
      "finished with sample 35\n",
      "finished with sample 36\n",
      "finished with sample 37\n",
      "finished with sample 38\n",
      "finished with sample 39\n",
      "finished with sample 40\n",
      "finished with sample 41\n",
      "finished with sample 42\n",
      "finished with sample 43\n",
      "finished with sample 44\n",
      "finished with sample 45\n",
      "finished with sample 46\n",
      "finished with sample 47\n",
      "finished with sample 48\n",
      "finished with sample 49\n",
      "finished with sample 50\n",
      "finished with sample 51\n",
      "finished with sample 52\n",
      "finished with sample 53\n",
      "finished with sample 54\n",
      "finished with sample 55\n",
      "finished with sample 56\n",
      "finished with sample 57\n",
      "finished with sample 58\n",
      "finished with sample 59\n",
      "finished with sample 60\n",
      "finished with sample 61\n",
      "finished with sample 62\n",
      "finished with sample 63\n",
      "finished with sample 64\n",
      "finished with sample 1\n",
      "finished with sample 2\n",
      "finished with sample 3\n",
      "finished with sample 4\n",
      "finished with sample 5\n",
      "finished with sample 6\n",
      "finished with sample 7\n",
      "finished with sample 8\n",
      "finished with sample 9\n",
      "finished with sample 10\n",
      "finished with sample 11\n",
      "finished with sample 12\n",
      "finished with sample 13\n",
      "finished with sample 14\n",
      "finished with sample 15\n",
      "finished with sample 16\n",
      "finished with sample 17\n",
      "finished with sample 18\n",
      "finished with sample 19\n",
      "finished with sample 20\n",
      "finished with sample 21\n",
      "finished with sample 22\n",
      "finished with sample 23\n",
      "finished with sample 24\n",
      "finished with sample 25\n",
      "finished with sample 26\n",
      "finished with sample 27\n",
      "finished with sample 28\n",
      "finished with sample 29\n",
      "finished with sample 30\n",
      "finished with sample 31\n",
      "finished with sample 32\n",
      "finished with sample 33\n",
      "finished with sample 34\n",
      "finished with sample 35\n",
      "finished with sample 36\n",
      "finished with sample 37\n",
      "finished with sample 38\n",
      "finished with sample 39\n",
      "finished with sample 40\n",
      "finished with sample 41\n",
      "finished with sample 42\n",
      "finished with sample 43\n",
      "finished with sample 44\n",
      "finished with sample 45\n",
      "finished with sample 46\n",
      "finished with sample 47\n",
      "finished with sample 48\n",
      "finished with sample 49\n",
      "finished with sample 50\n",
      "finished with sample 51\n",
      "finished with sample 52\n",
      "finished with sample 53\n",
      "finished with sample 54\n",
      "finished with sample 55\n",
      "finished with sample 56\n",
      "finished with sample 57\n",
      "finished with sample 58\n",
      "finished with sample 59\n",
      "finished with sample 60\n",
      "finished with sample 61\n",
      "finished with sample 62\n",
      "finished with sample 63\n",
      "finished with sample 64\n",
      "finished with sample 1\n",
      "finished with sample 2\n",
      "finished with sample 3\n",
      "finished with sample 4\n",
      "finished with sample 5\n",
      "finished with sample 6\n",
      "finished with sample 7\n",
      "finished with sample 8\n",
      "finished with sample 9\n",
      "finished with sample 10\n",
      "finished with sample 11\n",
      "finished with sample 12\n",
      "finished with sample 13\n",
      "finished with sample 14\n",
      "finished with sample 15\n",
      "finished with sample 16\n",
      "finished with sample 17\n",
      "finished with sample 18\n",
      "finished with sample 19\n",
      "finished with sample 20\n",
      "finished with sample 21\n",
      "finished with sample 22\n",
      "finished with sample 23\n",
      "finished with sample 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with sample 25\n",
      "finished with sample 26\n",
      "finished with sample 27\n",
      "finished with sample 28\n",
      "finished with sample 29\n",
      "finished with sample 30\n",
      "finished with sample 31\n",
      "finished with sample 32\n",
      "finished with sample 33\n",
      "finished with sample 34\n",
      "finished with sample 35\n",
      "finished with sample 36\n",
      "finished with sample 37\n",
      "finished with sample 38\n",
      "finished with sample 39\n",
      "finished with sample 40\n",
      "finished with sample 41\n",
      "finished with sample 42\n",
      "finished with sample 43\n",
      "finished with sample 44\n",
      "finished with sample 45\n",
      "finished with sample 46\n",
      "finished with sample 47\n",
      "finished with sample 48\n",
      "finished with sample 49\n",
      "finished with sample 50\n",
      "finished with sample 51\n",
      "finished with sample 52\n",
      "finished with sample 53\n",
      "finished with sample 54\n",
      "finished with sample 55\n",
      "finished with sample 56\n",
      "finished with sample 57\n",
      "finished with sample 58\n",
      "finished with sample 59\n",
      "finished with sample 60\n",
      "finished with sample 61\n",
      "finished with sample 62\n",
      "finished with sample 63\n",
      "finished with sample 64\n",
      "finished with sample 1\n",
      "finished with sample 2\n",
      "finished with sample 3\n",
      "finished with sample 4\n",
      "finished with sample 5\n",
      "finished with sample 6\n",
      "finished with sample 7\n",
      "finished with sample 8\n",
      "finished with sample 9\n",
      "finished with sample 10\n",
      "finished with sample 11\n",
      "finished with sample 12\n",
      "finished with sample 13\n",
      "finished with sample 14\n",
      "finished with sample 15\n",
      "finished with sample 16\n",
      "finished with sample 17\n",
      "finished with sample 18\n",
      "finished with sample 19\n",
      "finished with sample 20\n",
      "finished with sample 21\n",
      "finished with sample 22\n",
      "finished with sample 23\n",
      "finished with sample 24\n",
      "finished with sample 25\n",
      "finished with sample 26\n",
      "finished with sample 27\n",
      "finished with sample 28\n",
      "finished with sample 29\n",
      "finished with sample 30\n",
      "finished with sample 31\n",
      "finished with sample 32\n",
      "finished with sample 33\n",
      "finished with sample 34\n",
      "finished with sample 35\n",
      "finished with sample 36\n",
      "finished with sample 37\n",
      "finished with sample 38\n",
      "finished with sample 39\n",
      "finished with sample 40\n",
      "finished with sample 41\n",
      "finished with sample 42\n",
      "finished with sample 43\n",
      "finished with sample 44\n",
      "finished with sample 45\n",
      "finished with sample 46\n",
      "finished with sample 47\n",
      "finished with sample 48\n",
      "finished with sample 49\n",
      "finished with sample 50\n",
      "finished with sample 51\n",
      "finished with sample 52\n",
      "finished with sample 53\n",
      "finished with sample 54\n",
      "finished with sample 55\n",
      "finished with sample 56\n",
      "finished with sample 57\n",
      "finished with sample 58\n",
      "finished with sample 59\n",
      "finished with sample 60\n",
      "finished with sample 61\n",
      "finished with sample 62\n",
      "finished with sample 63\n",
      "finished with sample 64\n",
      "finished with sample 1\n",
      "finished with sample 2\n",
      "finished with sample 3\n",
      "finished with sample 4\n",
      "finished with sample 5\n",
      "finished with sample 6\n",
      "finished with sample 7\n",
      "finished with sample 8\n",
      "finished with sample 9\n",
      "finished with sample 10\n",
      "finished with sample 11\n",
      "finished with sample 12\n",
      "finished with sample 13\n",
      "finished with sample 14\n",
      "finished with sample 15\n",
      "finished with sample 16\n",
      "finished with sample 17\n",
      "finished with sample 18\n",
      "finished with sample 19\n",
      "finished with sample 20\n",
      "finished with sample 21\n",
      "finished with sample 22\n",
      "finished with sample 23\n",
      "finished with sample 24\n",
      "finished with sample 25\n",
      "finished with sample 26\n",
      "finished with sample 27\n",
      "finished with sample 28\n",
      "finished with sample 29\n",
      "finished with sample 30\n",
      "finished with sample 31\n",
      "finished with sample 32\n",
      "finished with sample 33\n",
      "finished with sample 34\n",
      "finished with sample 35\n",
      "finished with sample 36\n",
      "finished with sample 37\n",
      "finished with sample 38\n",
      "finished with sample 39\n",
      "finished with sample 40\n",
      "finished with sample 41\n",
      "finished with sample 42\n",
      "finished with sample 43\n",
      "finished with sample 44\n",
      "finished with sample 45\n",
      "finished with sample 46\n",
      "finished with sample 47\n",
      "finished with sample 48\n",
      "finished with sample 49\n",
      "finished with sample 50\n",
      "finished with sample 51\n",
      "finished with sample 52\n",
      "finished with sample 53\n",
      "finished with sample 54\n",
      "finished with sample 55\n",
      "finished with sample 56\n",
      "finished with sample 57\n",
      "finished with sample 58\n",
      "finished with sample 59\n",
      "finished with sample 60\n",
      "finished with sample 61\n",
      "finished with sample 62\n",
      "finished with sample 63\n",
      "finished with sample 64\n",
      "finished with sample 1\n",
      "finished with sample 2\n",
      "finished with sample 3\n",
      "finished with sample 4\n",
      "finished with sample 5\n",
      "finished with sample 6\n",
      "finished with sample 7\n",
      "finished with sample 8\n",
      "finished with sample 9\n",
      "finished with sample 10\n",
      "finished with sample 11\n",
      "Finished predictions\n"
     ]
    }
   ],
   "source": [
    "trainer.load('./conv_experiments/train_3by3_no_norm_on_spell_with_pretrain_0.7tf1543267208/listener-29.pt', \n",
    "             './conv_experiments/train_3by3_no_norm_on_spell_with_pretrain_0.7tf1543267208/speller-29.pt')\n",
    "# trainer.update_lr(1e-4)\n",
    "\n",
    "# trainer.val(pre_trained=False, plot=True)\n",
    "trainer.test()\n",
    "# trainer.load(None, './experiments/pretrain_with_learned_hidden1543130180/speller-5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # best_error = 1e30  # set to super large value at first\n",
    "# # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(trainer.optimizer, 'min', factor=0.3, patience=1, verbose=True)\n",
    "# # scheduler = torch.optim.lr_scheduler.StepLR(trainer.optimizer, step_size=5, gamma=0.3)\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "# #     scheduler.step()\n",
    "#     trainer.train(pre_train=PRE_TRAIN, teacher_force=0.7)\n",
    "#     error = trainer.val(pre_trained=PRE_TRAIN)\n",
    "#     scheduler.step(error)\n",
    "#     if error < best_error:\n",
    "#         best_error = error\n",
    "#         print(\"Saving model, predictions and generated output for epoch \" + \n",
    "#                 str(epoch)+\" with Error: \" + str(np.exp(best_error)))\n",
    "#         trainer.save(pre_trained=PRE_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training curves\n",
    "plt.figure()\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation NLL')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('NLL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
